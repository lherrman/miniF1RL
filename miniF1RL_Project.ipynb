{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TuIT_DeepRL - DRL Project\n",
    "![OST](./resources/ost_logo.png)\n",
    "## MiniF1RL: My pygame 2d racing environment for DRL\n",
    "Author: Lars Herrmann    \n",
    "Date: 12.04.2024  \n",
    "Repository: [Github - miniF1RL](https://github.com/lherrman/miniF1RL)   \n",
    "Python Version: 3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "After a lot of experimentaition and consideration of what environment to use. (Playing with e.g. TrackManiaRL, Gymnasium Integrated Environemnts and Unity Environments), i decided to implement my own 2d racing environment. The main reason for this is that i wanted a environment where no pre-trained models are available what motivates me to train my own models. As i started watching formula one this season, i wanted to go with a racing environment. \n",
    "\n",
    "## Implementing the environment\n",
    "\n",
    "### The CarModel\n",
    "As a starting point, i was able to use a Python class of a simple 2d topdown car model that i implemented for another project about a year ago. The class already had methods to draw a simple 2d box car and it's wheels onto a pygame screen. As it wasn't intended as a racing game, and rather a geometrical model to controll an rc car, i had to implement an updated method for updating the cars position and velocity to make it more fun to drive. The original CarModel class can be found in my github repository for the [SlamCar Project](https://github.com/lherrman/slamcar-controller) in the file `base_model.py`. \n",
    "\n",
    "To make this into a racing environment, there were still some parts missing i startet to implement, starting with a track to drive on.\n",
    "\n",
    "As of now, not a lot of the original code is left.\n",
    "\n",
    "### The Track\n",
    "In order to have a track to drive on, i decided to draw a black and white image of a track in Photoshop. When initializing the car model, i wrote a python method that uses the 'cv2.findContours' method to find the track boundaries in the image. The track is then represented as two lists of points, one for the inner and one for the outer boundary.\n",
    "\n",
    "![Track Image](./resources/MakeTrack.png)\n",
    "\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "    def _get_track_boundaries_from_image(self, image_path) -> dict:\n",
    "        '''\n",
    "        Using opencv to read the image and find the contours of the track boundaries.\n",
    "        The boundaries are represented as list of points. ([x1, y1], [x2, y2], ...)\n",
    "        '''\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        contours, _ = cv2.findContours(image.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        assert len(contours) == 2, \"There should be exactly 2 contours in the image\"\n",
    "\n",
    "        # Transform and downsample the contours\n",
    "        downsample_rate = 10\n",
    "        inner_boundary = contours[0][:, 0, :][::downsample_rate]\n",
    "        outer_boundary = contours[1][:, 0, :][::downsample_rate]\n",
    "\n",
    "        # # Append last point to close the loop\n",
    "        inner_boundary = np.append(inner_boundary, [inner_boundary[0]], axis=0)\n",
    "        outer_boundary = np.append(outer_boundary, [outer_boundary[0]], axis=0)\n",
    "\n",
    "        # Scale the contours to make suitable for the simulation\n",
    "        inner_boundary, outer_boundary = inner_boundary / 60, outer_boundary / 60\n",
    "\n",
    "        return {\n",
    "            'inner': inner_boundary,\n",
    "            'outer': outer_boundary\n",
    "        }   \n",
    "```\n",
    "\n",
    "#### Track progress\n",
    "To approximate the progress the car has made on the track, i implemented a method `_calculate_track_progress` that calculates the distance to the next point on the inner boundary. The first point on the inner boundary is set as the starting point. The progress is then calculated as the index of the nearest point divided by the total number of points on the inner boundary. This could be improved by taking the length of the segments passed into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The observation space\n",
    "\n",
    "I wanted to keep the observation space simple, so i decided to use 'lidar sensors', that give the car only a few distance measurements in front of it. I decided to use 3 sensors, one in the middle and one on each side of the car in a 45 degree angle. The sensors are implemented as a method of the CarModel class that returns the distances to the track boundaries. \n",
    "\n",
    "The implementation of the `_segment_intersection` and `_raycast` methods was prompted to ChatGPT, which returned an working implementation after 2-3 iterations. The method uses an all python implementation for which the performance isn't optimal, but i guess it's good enough for my purposes. As the raycast method has to itterate over each segment of the boundaries, a more performant implementation would be beneficial.\n",
    "\n",
    "For easier validation, and because it looks cool, i decided to draw the lidar sensors as lines on the pygame screen.\n",
    "\n",
    "\n",
    "![Lidar Sensors](./resources/lidars.png)\n",
    "\n",
    "In addition to the 3 lidar sensors, i also added the cars steering angle to the observation space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The action space\n",
    "\n",
    "The action space is also very simple. To make the problem even easier for the RL algorithm, i decided to always have the car accelerate, as slowing down isn't necessary in this environment. The car can steer left or right, or go straight. To make it a little more interesting, i added a third action, boost, that allows the car to go faster on the straights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reward function\n",
    "\n",
    "I decided the use the following 4 types of rewards:\n",
    "- Progress reward: The car gets a reward for making progress on the track\n",
    "- Speed reward: The car gets a reward for driving fast\n",
    "- Finish reward: The car gets a reward for finishing the track\n",
    "- Collision reward: The car gets a negative reward for colliding with the track boundaries\n",
    "\n",
    "As the rewards are calculated from differently scaled values, the weigths of progress and speed can't be compared directly. After some testing, i decided to use the following weights for the rewards:\n",
    "- Progress: 1000 (* fracton of the progress made in the last step)\n",
    "- Speed: 1 (* speed in m/s)\n",
    "- Finish: 100 (* 1 if the car has finished the track)\n",
    "- Collision: -100 (* 1 if the car has collided with the track boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Graphics\n",
    "To enhance the graphics of the environment, i generated a background image based on the drawn track by locally running a stable diffusion model and prompting it with the track image and a text description of the track i want.\n",
    "\n",
    "I used the JuggernautXL model trough the [Fooocus UI](https://github.com/lllyasviel/Fooocus).  \n",
    "I got the best results by setting the image prompt mode to CPDS, in order to keep the structure of the track. By decreasing the weight of the image prompt, the track structure is kept, but the model is free to generate a more interesting background.\n",
    "\n",
    "![Image Gen](./resources/fooocus_image_gen.png)\n",
    "\n",
    "Also for the car i generated a sprite image that is used to draw the car on the screen.   \n",
    "Note: I also got some quite funny results:  \n",
    "![Car Gen](./resources/car_fail.png)\n",
    "\n",
    "The environment allows to switch between the simple wireframe graphics and the enhanced graphics by pressing the 'g' key.\n",
    "I also added the possibility to zoom in and out using the mouse wheel. When the whole track is visible the camera will center on the track instead of the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo animation\n",
    "![Demo Animation](./resources/Trained_Env_Demo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import torch\n",
    "import pygame as pg \n",
    "from minif1env import MiniF1RLEnv\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_writer(texts: dict):\n",
    "    writer = SummaryWriter()\n",
    "    for key, value in texts.items():\n",
    "        writer.add_text(str(key), str(value))\n",
    "    return writer\n",
    "\n",
    "# Use a separate thread to render the environment\n",
    "# Avoids crashing the pg window during training\n",
    "def render_loop(env: MiniF1RLEnv, \n",
    "           close_event, \n",
    "           human_control=False, \n",
    "           fps=60):\n",
    "    pg.init()\n",
    "    while True:\n",
    "        delay_ms = 1000 // fps\n",
    "        env.render()\n",
    "        pg.time.wait(delay_ms) \n",
    "\n",
    "        done = env.handle_pg_events(human_control=human_control)\n",
    "\n",
    "        if done:\n",
    "           close_event.set()\n",
    "\n",
    "        if close_event.is_set():\n",
    "            pg.quit()\n",
    "            return\n",
    "        \n",
    "def latest_checkpoint_file_path(model_description: str):\n",
    "  checkpoint_dir = \"checkpoints\"\n",
    "  checkpoint_files =  glob.glob(os.path.join(checkpoint_dir, f\"*{model_description}*/*.zip\"))\n",
    "  if not checkpoint_files:\n",
    "    return None\n",
    "  latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "  return latest_checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Play with the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MiniF1RLEnv()\n",
    "\n",
    "done = False\n",
    "last_frame = datetime.now()\n",
    "while not done:\n",
    "\n",
    "    # Limit framerate for manual controlls\n",
    "    if (datetime.now() - last_frame).total_seconds() < 1/100:\n",
    "        continue\n",
    "    last_frame = datetime.now()\n",
    "\n",
    "    exit = env.handle_pg_events(human_control=True)\n",
    "\n",
    "    observation, step_reward, terminate, info, idk = env.step(None)\n",
    "    env.render()\n",
    "\n",
    "    if terminate:\n",
    "        env.reset()\n",
    "\n",
    "    if exit:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the environment\n",
    "env = MiniF1RLEnv()\n",
    "\n",
    "# Create and start the rendering thread\n",
    "close_event = threading.Event()\n",
    "render_thread = threading.Thread(target=render_loop, args=(env,close_event,False,30))\n",
    "render_thread.start()\n",
    "\n",
    "# Set the training parameters\n",
    "total_timesteps = 1_000_000\n",
    "model_description = \"PPO_MlpPolicy_64_64_R7\"\n",
    "load_checkpoint = True\n",
    "\n",
    "# Load the latest checkpoint if available, otherwise create a new model\n",
    "latest_checkpoint = latest_checkpoint_file_path(model_description)\n",
    "if load_checkpoint and latest_checkpoint:\n",
    "  print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "  model = PPO.load(latest_checkpoint, env, verbose=1)\n",
    "else:\n",
    "  model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "run_name = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + model_description\n",
    "\n",
    "# Define the policy network and enable GPU usage\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[64, 64])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tensorboard writer\n",
    "writer = get_writer({\"run_name\": run_name,\n",
    "                    \"model_description\": model_description,\n",
    "                    \"total_timesteps\": total_timesteps,\n",
    "                    \"env_name\": \"MiniF1RLEnv\",\n",
    "                    \"reward_weights\": env.get_reward_weights(),\n",
    "                    \"device\": device,\n",
    "                    \"policy_kwargs\": policy_kwargs})\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10_000,\n",
    "  save_path=f\"./checkpoints/{run_name}\",\n",
    "  name_prefix=f\"{run_name}_checkpoint\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=total_timesteps, \n",
    "            tb_log_name=run_name, \n",
    "            callback=checkpoint_callback)\n",
    "model.save(f\"models/{run_name}\")\n",
    "\n",
    "# Close the rendering thread and environment\n",
    "close_event.set()\n",
    "render_thread.join()\n",
    "env.close()\n",
    "\n",
    "# Close the tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints\\20240507_220914_PPO_MlpPolicy_64_64_R7\\20240507_220914_PPO_MlpPolicy_64_64_R7_checkpoint_90000_steps.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_description = \"PPO_MlpPolicy_64_64_R7\"\n",
    "env = MiniF1RLEnv()\n",
    "\n",
    "# Load the latest checkpoint that matches the model description\n",
    "latest_checkpoint = latest_checkpoint_file_path(model_description)\n",
    "if not latest_checkpoint:\n",
    "    raise ValueError(f\"No checkpoint found for model description: {model_description}\")\n",
    "print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "model = PPO.load(latest_checkpoint)\n",
    "\n",
    "# Create and start the rendering thread\n",
    "close_event = threading.Event()\n",
    "render_thread = threading.Thread(target=render_loop, args=(env,close_event,False,30))\n",
    "render_thread.start()\n",
    "\n",
    "# Run the model\n",
    "exit = False\n",
    "while not exit:\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info, idk = env.step(action)\n",
    "\n",
    "        if close_event.is_set():\n",
    "            exit = True\n",
    "            break\n",
    "\n",
    "# Close the rendering thread and environment\n",
    "close_event.set()\n",
    "render_thread.join()\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Log\n",
    "These are rougly the different stages at which i run the training.\n",
    "\n",
    "## Tests 01\n",
    "**Best Model:**  \"a2c_minif1rl\"   \n",
    "**Timesteps**: 10'000   \n",
    "**Total Training Time:**  ~1h  \n",
    "**Algorithm:** A2C  \n",
    "**Conditions:** After every crash, the car is reset to the starting position.\n",
    "**Result:** The car learned to always directly drive into the right border.     \n",
    "Explaination. After checking the rewards that were given to the model, i realized that the reward function was not working as intended. The weight for the progress reward was way to low so the model did not learn to follow the track. Probebly also the punishment for hitting the track boundaries was to low.\n",
    "\n",
    "## Tests 02\n",
    "**Best Model:**  \"PPO_MlpPolicy_64_64_R4\"   \n",
    "**Timesteps**: 10'000  \n",
    "**Total Training Time:**  ~1h   \n",
    "**Algorithm:** PPO  \n",
    "**Conditions:** After every crash, the car is reset to the starting position. I added some slight randomness to the starting position and heading of the car.   \n",
    "**Result:** Still in most of the runs the car learned to drive into the right border. He never came to the point to get to the first left turn.\n",
    "\n",
    "## Tests 03\n",
    "**Best Model:**  \"PPO_MlpPolicy_64_64_R7\"   \n",
    "**Timesteps**: 10'000  \n",
    "**Total Training Time:**  ~1h  \n",
    "**Algorithm:** PPO  \n",
    "**Conditions:** (New reset function implemented) After every crash, the car is reset to a random position in the center of the track heading into track direction.    \n",
    "**Result:** The car learned to drive on the track. On the best run, after about 10 minutes it learned to do the right curves pretty consistently. The left curves were still a problem until about 30 minutes into the training. I experience quite a large difference in learning speed between the different runs. Some runs never learned to drive on the track properly as the started to consistenlty drive into the right border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
